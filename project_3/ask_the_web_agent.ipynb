{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f7fe8ac",
   "metadata": {},
   "source": [
    "# Project 3: **Ask‑the‑Web Agent**\n",
    "\n",
    "Welcome to Project 3! In this project, you will learn how to use tool‑calling LLMs, extend them with custom tools, and build a simplified *Perplexity‑style* agent that answers questions by searching the web."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4311a6",
   "metadata": {},
   "source": [
    "## Learning Objectives  \n",
    "* Understand why tool calling is useful and how LLMs can invoke external tools.\n",
    "* Implement a minimal loop that parses the LLM's output and executes a Python function.\n",
    "* See how *function schemas* (docstrings and type hints) let us scale to many tools.\n",
    "* Use **LangChain** to get function‑calling capability for free (ReAct reasoning, memory, multi‑step planning).\n",
    "* Combine LLM with a web‑search tool to build a simple ask‑the‑web agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f16864",
   "metadata": {},
   "source": [
    "## Roadmap\n",
    "1. Environment setup\n",
    "2. Write simple tools and connect them to an LLM\n",
    "3. Standardize tool calling by writing `to_schema`\n",
    "4. Use LangChain to augment an LLM with your tools\n",
    "5. Build a Perplexity‑style web‑search agent\n",
    "6. (Optional) A minimal backend and frontend UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae51d0",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "# 1- Environment setup\n",
    "\n",
    "## 1.1- Conda environment\n",
    "\n",
    "Before we start coding, you need a reproducible setup. Open a terminal in the same directory as this notebook and run:\n",
    "\n",
    "```bash\n",
    "# Create and activate the conda environment\n",
    "conda env create -f environment.yml && conda activate web_agent\n",
    "\n",
    "# Register this environment as a Jupyter kernel\n",
    "python -m ipykernel install --user --name=web_agent --display-name \"web_agent\"\n",
    "```\n",
    "Once this is done, you can select “web_agent” from the Kernel → Change Kernel menu in Jupyter or VS Code.\n",
    "\n",
    "\n",
    "> Behind the scenes:\n",
    "> * Conda reads `environment.yml`, resolves the pinned dependencies, creates an isolated environment named `web_agent`, and activates it.\n",
    "> * `ollama pull` downloads the model so you can run it locally without API calls.\n",
    "\n",
    "\n",
    "## 1.2 Ollama setup\n",
    "\n",
    "In this project, we start with `gemma3-1B` because it is lightweight and runs on most machines. You can try other smaller or larger LLMs such as `mistral:7b`, `phi3:mini`, or `llama3.2:1b` to compare performance. Explore available models here: https://ollama.com/library\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.2:3b\n",
    "```\n",
    "\n",
    "`ollama pull` downloads the model so you can run it locally without API calls.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c27158f",
   "metadata": {},
   "source": [
    "## 2- Tool Calling\n",
    "\n",
    "LLMs are strong at answering questions, but they cannot directly access external data such as live web results, APIs, or computations. In real applications, agents rarely rely only on their internal knowledge. They need to query APIs, retrieve data, or perform calculations to stay accurate and useful. Tool calling bridges this gap by allowing the LLM to request actions from the outside world.\n",
    "\n",
    "\n",
    "We describe each tool’s interface in the model’s prompt, defining what it does and what arguments it expects. When the model decides that a tool is needed, it emits a structured output like: `TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"San Francisco\"}}`. Your code will detect this output, execute the corresponding function, and feed the result back to the LLM so the conversation continues.\n",
    "\n",
    "In this section, you will implement a simple `get_current_weather` function and teach the `gemma3` model how to use it when required in four steps:\n",
    "1. Implement the tool\n",
    "2. Create the instructions for the LLM\n",
    "3. Call the LLM with the prompt\n",
    "4. Parse the LLM output and call the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a536f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 1: Implement the tool\n",
    "# ---------------------------------------------------------\n",
    "# Your goal: give the model a way to access weather information.\n",
    "# You can either:\n",
    "#   (a) Call a real weather API (for example, OpenWeatherMap), or\n",
    "#   (b) Create a dummy function that returns a fixed response (e.g., \"It is 23°C and sunny in San Francisco.\")\n",
    "#\n",
    "# Requirements:\n",
    "#   • The function should be named `get_current_weather`\n",
    "#   • It should take two arguments:\n",
    "#         - city: str\n",
    "#         - unit: str = \"celsius\"\n",
    "#   • Return a short, human-readable sentence describing the weather.\n",
    "#\n",
    "# Example expected behavior:\n",
    "#   get_current_weather(\"San Francisco\") → \"It is 23°C and sunny in San Francisco.\"\n",
    "#\n",
    "\n",
    "\n",
    "def get_current_weather_dummy(city: str, unit: str = \"celsius\") -> str:\n",
    "    temperature = \"23°C\" if unit == \"celsius\" else \"73°F\"\n",
    "    return f\"It is {temperature} and sunny in {city}.\"\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "API_KEY = \"2733b92981329a239d31a756bb055903\"  # Replace with your actual key\n",
    "\n",
    "def get_current_weather(city: str, unit: str = \"celsius\") -> str:\n",
    "    # Map unit to OpenWeatherMap format\n",
    "    units_map = {\"celsius\": \"metric\", \"fahrenheit\": \"imperial\"}\n",
    "    units = units_map.get(unit.lower(), \"metric\")\n",
    "\n",
    "    url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={API_KEY}&units={units}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            # Show more detailed error info\n",
    "            try:\n",
    "                error_data = response.json()\n",
    "                error_msg = error_data.get('message', 'Unknown error')\n",
    "                return f\"Could not retrieve weather for {city}. API Error ({response.status_code}): {error_msg}\"\n",
    "            except:\n",
    "                return f\"Could not retrieve weather for {city}. Status code: {response.status_code}\"\n",
    "        \n",
    "        data = response.json()\n",
    "        temp = data[\"main\"][\"temp\"]\n",
    "        description = data[\"weather\"][0][\"description\"]\n",
    "        return f\"It is {temp}°{'C' if units=='metric' else 'F'} and {description} in {city}.\"\n",
    "    \n",
    "    except requests.exceptions.Timeout:\n",
    "        return f\"Request timed out while getting weather for {city}.\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Network error getting weather for {city}: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a51ab3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing weather API directly...\n",
      "Result: It is 6.36°C and light rain in Seattle,US.\n",
      "\n",
      "Testing URL: http://api.openweathermap.org/data/2.5/weather?q=Seattle&appid=2733b92981329a239d31a756bb055903&units=metric\n",
      "Status Code: 200\n"
     ]
    }
   ],
   "source": [
    "# Test the weather API directly to see the exact error\n",
    "print(\"Testing weather API directly...\")\n",
    "result = get_current_weather(\"Seattle,US\", unit=\"celsius\")\n",
    "print(f\"Result: {result}\")\n",
    "\n",
    "# Also test the URL directly\n",
    "url = f\"http://api.openweathermap.org/data/2.5/weather?q=Seattle&appid={API_KEY}&units=metric\"\n",
    "print(f\"\\nTesting URL: {url}\")\n",
    "response = requests.get(url)\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "if response.status_code != 200:\n",
    "    print(f\"Error Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a43c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 2: Create the prompt for the LLM to call tools\n",
    "# ---------------------------------------------------------\n",
    "# Goal:\n",
    "#   Build the system and user prompts that instruct the model when and how\n",
    "#   to use your tool (`get_current_weather`).\n",
    "#\n",
    "# What to include:\n",
    "#   • A SYSTEM_PROMPT that tells the model about the tool use and describe the tool\n",
    "#   • A USER_QUESTION with a user query that should trigger the tool.\n",
    "#       Example: \"What is the weather in San Diego today?\"\n",
    "\n",
    "# Try experimenting with different system and user prompts\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 2: Create the prompt for the LLM to call tools\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an assistant that can answer questions and call tools when needed.\n",
    "\n",
    "Available tool:\n",
    "- get_current_weather(city: str, unit: str = \"celsius\") → Returns a short sentence describing the weather.\n",
    "\n",
    "When you need to use a tool, output ONLY in this format:\n",
    "TOOL_CALL: {\"name\": \"<tool_name>\", \"args\": {\"city\": \"<city>\", \"unit\": \"<unit>\"}}\n",
    "\n",
    "Example:\n",
    "TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"San Francisco\", \"unit\": \"celsius\"}}\n",
    "\n",
    "Do NOT include any extra text outside this JSON format when calling a toolDo NOT include any extra text outside this JSON format when calling a tool.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c67a736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Output: TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"San Diego\", \"unit\": \"celsius\"}}\n",
      "Tool Result: It is 24.73°C and clear sky in San Diego.\n",
      "Tool Result: It is 24.73°C and clear sky in San Diego.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 2: Create the prompt for the LLM to call tools\n",
    "# ---------------------------------------------------------\n",
    "# Goal:\n",
    "#   Build the system and user prompts that instruct the model when and how\n",
    "#   to use your tool (`get_current_weather`).\n",
    "#\n",
    "# What to include:\n",
    "#   • A SYSTEM_PROMPT that tells the model about the tool use and describe the tool\n",
    "#   • A USER_QUESTION with a user query that should trigger the tool.\n",
    "#       Example: \"What is the weather in San Diego today?\"\n",
    "\n",
    "# Try experimenting with different system and user prompts\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 2: Create the prompt for the LLM to call tools\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "USER_QUESTION = \"What is the weather in San Diego today?\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Call the LLM with the prompts\n",
    "# ---------------------------------------------------------\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3.2:3b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_QUESTION}\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm_output = response.choices[0].message.content\n",
    "print(\"LLM Output:\", llm_output)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Parse the LLM output and call the tool from Step 1\n",
    "# ---------------------------------------------------------\n",
    "if llm_output.startswith(\"TOOL_CALL:\"):\n",
    "    tool_call = json.loads(llm_output.replace(\"TOOL_CALL:\", \"\").strip())\n",
    "    if tool_call[\"name\"] == \"get_current_weather\":\n",
    "        city = tool_call[\"args\"][\"city\"]\n",
    "        unit = tool_call[\"args\"].get(\"unit\", \"celsius\")\n",
    "        \n",
    "        # Call the function from Step 1 (real API or dummy)\n",
    "        tool_result = get_current_weather(city, unit)\n",
    "        print(\"Tool Result:\", tool_result)\n",
    "    else:\n",
    "               print(\"Unknown tool requested.\")\n",
    "else:\n",
    "    print(\"No tool call detected in LLM output.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eebb062",
   "metadata": {},
   "source": [
    "Now that you have defined a tool and shown the model how to use it, the next step is to call the LLM using your prompt.\n",
    "\n",
    "Start the **Ollama** server in a terminal with `ollama serve`. This launches a local API endpoint that listens for LLM requests. Once the server is running, return to the notebook and in the next cell send a query to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "027cb75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-46', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"San Diego\", \"unit\": \"celsius\"}}', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1764106562, model='llama3.2:3b', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=30, prompt_tokens=179, total_tokens=209, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "LLM Output: TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"San Diego\", \"unit\": \"celsius\"}}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key = \"ollama\", base_url = \"http://localhost:11434/v1\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 3: Call the LLM with your prompt\n",
    "# ---------------------------------------------------------\n",
    "# Task:\n",
    "#   Send SYSTEM_PROMPT + USER_QUESTION to the model.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Use the Ollama client to create a chat completion. \n",
    "#       - You may find some examples here: https://platform.openai.com/docs/api-reference/chat/create\n",
    "#       - If you are unsure, search the web for \"client.chat.completions.create\"\n",
    "#   2. Print the raw response.\n",
    "#\n",
    "# Expected:\n",
    "#   The model should return something like:\n",
    "#   TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"San Diego\"}}\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 3: Call the LLM with your prompt\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Create a chat completion using the Ollama-compatible OpenAI client\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3.2:3b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_QUESTION},\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Print the raw response object\n",
    "print(response)\n",
    "\n",
    "# Optionally, also print the assistant's message content for convenience\n",
    "print(\"LLM Output:\", response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "94aeb4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling tool `get_current_weather` with args {'city': 'San Diego', 'unit': 'celsius'}\n",
      "Result: It is 24.73°C and clear sky in San Diego.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 4: Parse the LLM output and call the tool\n",
    "# ---------------------------------------------------------\n",
    "# Task:\n",
    "#   Detect when the model requests a tool, extract its name and arguments,\n",
    "#   and execute the corresponding function.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Search for the text pattern \"TOOL_CALL:{...}\" in the model output.\n",
    "#   2. Parse the JSON inside it to get the tool name and args.\n",
    "#   3. Call the matching function (e.g., get_current_weather).\n",
    "#\n",
    "# Expected:\n",
    "#   You should see a line like:\n",
    "#       Calling tool `get_current_weather` with args {'city': 'San Diego'}\n",
    "#       Result: It is 23°C and sunny in San Diego.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import re, json\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 4: Parse the LLM output and call the tool\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Look for a TOOL_CALL followed by a JSON object\n",
    "match = re.search(r\"TOOL_CALL:\\s*(\\{.*\\})\", llm_output, re.DOTALL)\n",
    "if match:\n",
    "    tool_json_str = match.group(1)\n",
    "    try:\n",
    "        tool_call = json.loads(tool_json_str)\n",
    "        name = tool_call.get(\"name\")\n",
    "        args = tool_call.get(\"args\", {})\n",
    "        print(f\"Calling tool `{name}` with args {args}\")\n",
    "\n",
    "        if name == \"get_current_weather\":\n",
    "            city = args.get(\"city\")\n",
    "            unit = args.get(\"unit\", \"celsius\")\n",
    "            result = get_current_weather(city, unit)\n",
    "            print(f\"Result: {result}\")\n",
    "        else:\n",
    "            print(f\"Unknown tool: {name}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to parse TOOL_CALL JSON: {e}\")\n",
    "else:\n",
    "    # If no tool call is found, output the model's response directly\n",
    "    print(\"No TOOL_CALL found. LLM Response:\")\n",
    "    print(llm_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be26661",
   "metadata": {},
   "source": [
    "# 3- Standadize tool calling\n",
    "\n",
    "So far, we handled tool calling manually by writing one regex and one hard-coded function. This approach does not scale if we want to add more tools. Adding more tools would mean more `if/else` blocks and manual edits to the `TOOL_SPEC` prompt.\n",
    "\n",
    "To make the system flexible, we can standardize tool definitions by automatically reading each function’s signature, converting it to a JSON schema, and passing that schema to the LLM. This way, the LLM can dynamically understand which tools exist and how to call them without requiring manual updates to prompts or conditional logic.\n",
    "\n",
    "Next, you will implement a small helper that extracts metadata from functions and builds a schema for each tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce911b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'args': [{'default': None, 'name': 'city', 'type': \"<class 'str'>\"},\n",
      "          {'default': 'celsius', 'name': 'unit', 'type': \"<class 'str'>\"}],\n",
      " 'description': 'No description available',\n",
      " 'name': 'get_current_weather'}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Generate a JSON schema for a tool automatically\n",
    "# ---------------------------------------------------------\n",
    "#\n",
    "# Steps:\n",
    "#   1. Use `inspect.signature` to get function parameters.\n",
    "#   2. For each argument, record its name, type, and description.\n",
    "#   3. Build a schema containing:\n",
    "#   4. Test your helper on `get_current_weather` and print the result.\n",
    "#\n",
    "# Expected:\n",
    "#   A dictionary describing the tool (its name, args, and types).\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Generate a JSON schema for a tool automatically\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "from pprint import pprint\n",
    "import inspect\n",
    "\n",
    "def to_schema(fn):\n",
    "    schema = {\n",
    "        \"name\": fn.__name__,\n",
    "        \"description\": fn.__doc__ or \"No description available\",\n",
    "        \"args\": []\n",
    "    }\n",
    "\n",
    "    sig = inspect.signature(fn)\n",
    "    for param_name, param in sig.parameters.items():\n",
    "        arg_info = {\n",
    "            \"name\": param_name,\n",
    "            \"type\": str(param.annotation) if param.annotation != inspect._empty else \"string\",\n",
    "            \"default\": param.default if param.default != inspect._empty else None\n",
    "        }\n",
    "        schema[\"args\"].append(arg_info)\n",
    "\n",
    "    return schema\n",
    "\n",
    "# Test on# Test on get_current_weather\n",
    "tool_schema = to_schema(get_current_weather)\n",
    "pprint(tool_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6cc25b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'args': [{'default': None, 'name': 'city', 'type': \"<class 'str'>\"},\n",
      "          {'default': 'celsius', 'name': 'unit', 'type': \"<class 'str'>\"}],\n",
      " 'description': 'No description available',\n",
      " 'name': 'get_current_weather'}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Generate a JSON schema for a tool automatically\n",
    "# ---------------------------------------------------------\n",
    "#\n",
    "# Steps:\n",
    "#   1. Use `inspect.signature` to get function parameters.\n",
    "#   2. For each argument, record its name, type, and description.\n",
    "#   3. Build a schema containing:\n",
    "#   4. Test your helper on `get_current_weather` and print the result.\n",
    "#\n",
    "# Expected:\n",
    "#   A dictionary describing the tool (its name, args, and types).\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "from pprint import pprint\n",
    "import inspect\n",
    "\n",
    "\n",
    "def to_schema(fn):\n",
    "    schema = {\n",
    "        \"name\": fn.__name__,\n",
    "        \"description\": fn.__doc__ or \"No description available\",\n",
    "        \"args\": []\n",
    "    }\n",
    "    sig = inspect.signature(fn)\n",
    "    for param_name, param in sig.parameters.items():\n",
    "        arg_info = {\n",
    "            \"name\": param_name,\n",
    "            \"type\": str(param.annotation) if param.annotation != inspect._empty else \"string\",\n",
    "            \"default\": param.default if param.default != inspect._empty else None\n",
    "        }\n",
    "        schema[\"args\"].append(arg_info)\n",
    "\n",
    "    return schema\n",
    "\n",
    "tool_schema = to_schema(get_current_weather)\n",
    "pprint(tool_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c1163f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Response: ChatCompletion(id='chatcmpl-969', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"San Diego\", \"unit\": \"celsius\"}}', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1764106601, model='llama3.2:3b', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=30, prompt_tokens=241, total_tokens=271, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "LLM Output: TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"San Diego\", \"unit\": \"celsius\"}}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Provide the tool schema to the model\n",
    "# ---------------------------------------------------------\n",
    "# Goal:\n",
    "#   Give the model a \"menu\" of available tools so it can choose\n",
    "#   which one to call based on the user’s question.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Add an extra system message (e.g., name=\"tool_spec\")\n",
    "#      containing the JSON schema(s) of your tools.\n",
    "#   2. Include SYSTEM_PROMPT and the user question as before.\n",
    "#   3. Send the messages to the model (e.g., llama3.2:3b).\n",
    "#   4. Print the raw model output to see if it picks the right tool.\n",
    "#\n",
    "# Expected:\n",
    "#   The model should produce a structured TOOL_CALL indicating\n",
    "#   which tool to use and with what arguments.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Provide the tool schema to the model\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Build the tool schema using the helper from previous step\n",
    "tool_schema = to_schema(get_current_weather)\n",
    "\n",
    "# Create messages for the LLM\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"system\", \"name\": \"tool_spec\", \"content\": json.dumps(tool_schema)},\n",
    "    {\"role\": \"user\", \"content\": USER_QUESTION}\n",
    "]\n",
    "\n",
    "# Send the messages to the model\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3.2:3b\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# Print raw response and the model's output\n",
    "print(\"Raw Response:\", response)\n",
    "print(\"LLM Output:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ec86e",
   "metadata": {},
   "source": [
    "## 4- LangChain for Tool Calling\n",
    "So far, you built a simple tool-calling pipeline manually. While this helps you understand the logic, it does not scale well when working with multiple tools, complex parsing, or multi-step reasoning.\n",
    "\n",
    "LangChain simplifies this process. You only need to declare your tools, and its *Agent* abstraction handles when to call a tool, how to use it, and how to continue reasoning afterward.\n",
    "\n",
    "In this section, you will use the **ReAct** Agent (Reasoning + Acting). It alternates between reasoning steps and tool use, producing clearer and more reliable results. We will explore reasoning-focused models in more depth next week.\n",
    "\n",
    "The following links might be helpful:\n",
    "- https://python.langchain.com/api_reference/langchain/agents/langchain.agents.initialize.initialize_agent.html\n",
    "- https://python.langchain.com/docs/integrations/tools/\n",
    "- https://python.langchain.com/docs/integrations/chat/ollama/\n",
    "- https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.llms.LLM.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c609d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 1: Define tools for LangChain\n",
    "# ---------------------------------------------------------\n",
    "# Goal:\n",
    "#   Convert your weather function into a LangChain-compatible tool.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Import `tool` from `langchain.tools`.\n",
    "#   2. Keep your existing `get_current_weather` helper as before.\n",
    "#   3. Create a new function (e.g., get_weather) that calls it.\n",
    "#   4. Add the `@tool` decorator so LangChain can register it automatically.\n",
    "#\n",
    "# Notes:\n",
    "#   • The decorator converts your Python function into a standardized tool object.\n",
    "#   • Start with keeping the logic simple and offline-friendly.\n",
    "\n",
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get the current weather for a city. Returns temperature in Celsius and weather conditions.\n",
    "    \n",
    "    Args:\n",
    "        city: The name of the city to get weather for (e.g., 'Seattle', 'San Francisco')\n",
    "    \n",
    "    Returns:\n",
    "        A string describing the current weather, including temperature and conditions.\n",
    "    \"\"\"\n",
    "    # Strip any quotes that the LLM might have included\n",
    "    city = city.strip().strip(\"'\\\"\")\n",
    "    \n",
    "    # Add country code for US cities to improve API lookup\n",
    "    # OpenWeatherMap API works better with \"City,CountryCode\" format\n",
    "    if \",\" not in city:\n",
    "        # Assume US cities for common names\n",
    "        city_with_country = f\"{city},US\"\n",
    "    else:\n",
    "        city_with_country = city\n",
    "    \n",
    "    # Call the real API function with proper formatting\n",
    "    result = get_current_weather(city_with_country, unit=\"celsius\")\n",
    "    \n",
    "    # If we got an error with the country code, try without it\n",
    "    if \"Could not retrieve weather\" in result and city_with_country != city:\n",
    "        result = get_current_weather(city, unit=\"celsius\")\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9552348d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using llama3.2:3b - a larger model with better ReAct capabilities\n",
      "Make sure you've pulled this model with: ollama pull llama3.2:3b\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mQuestion: Do I need an umbrella in Seattle today?\n",
      "\n",
      "Thought: To determine if I need an umbrella, I should get the current weather for Seattle.\n",
      "\n",
      "Action: get_weather\n",
      "Action Input: 'Seattle'\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIt is 6.36°C and light rain in Seattle,US.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: Do I need an umbrella in Seattle today?\n",
      "\n",
      "Thought: To determine if I need an umbrella, I should get the current weather for Seattle.\n",
      "\n",
      "Action: get_weather\n",
      "Action Input: 'Seattle'\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIt is 6.36°C and light rain in Seattle,US.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: Do I need an umbrella in Seattle today?\n",
      "\n",
      "Thought: To determine if I need an umbrella, I should check the weather conditions. Since it's light rain, I probably don't need an umbrella.\n",
      "\n",
      "Action: get_weather\n",
      "Action Input: 'Seattle'\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIt is 6.36°C and light rain in Seattle,US.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: Do I need an umbrella in Seattle today?\n",
      "\n",
      "Thought: To determine if I need an umbrella, I should check the weather conditions. Since it's light rain, I probably don't need an umbrella.\n",
      "\n",
      "Action: get_weather\n",
      "Action Input: 'Seattle'\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIt is 6.36°C and light rain in Seattle,US.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: No, you probably don't need an umbrella in Seattle today because it's light rain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "==================================================\n",
      "Final Result:\n",
      "No, you probably don't need an umbrella in Seattle today because it's light rain.\n",
      "\u001b[32;1m\u001b[1;3mFinal Answer: No, you probably don't need an umbrella in Seattle today because it's light rain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "==================================================\n",
      "Final Result:\n",
      "No, you probably don't need an umbrella in Seattle today because it's light rain.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 2: Initialize the LangChain Agent\n",
    "# ---------------------------------------------------------\n",
    "# Goal:\n",
    "#   Connect your tool to a local LLM using LangChain's ReAct-style agent.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Import the required classes:\n",
    "#        - ChatOllama (for local model access)\n",
    "#        - initialize_agent, Tool, AgentType\n",
    "#   2. Create an LLM instance (e.g., model=\"llama3.2:3b\", temperature=0).\n",
    "#   3. Add your tool(s) to a list\n",
    "#   4. Initialize the agent using initialize_agent\n",
    "#   5. Test the agent with a natural question (e.g., \"Do I need an umbrella in Seattle today?\").\n",
    "#\n",
    "# Expected:\n",
    "#   The model should reason through the question, call your tool,\n",
    "#   and produce a final answer in plain language.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 2: Initialize the LangChain Agent\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "print(\"Using llama3.2:3b - a larger model with better ReAct capabilities\")\n",
    "print(\"Make sure you've pulled this model with: ollama pull llama3.2:3b\\n\")\n",
    "\n",
    "# 1. Create an LLM instance using ChatOllama with a larger model\n",
    "llm = ChatOllama(model=\"llama3.2:3b\", temperature=0)\n",
    "\n",
    "# 2. Add your tool(s) to a list\n",
    "tools = [get_weather]  # This is the LangChain-compatible tool from Step 1\n",
    "\n",
    "# 3. Initialize the agent using ReAct style with error handling\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    max_iterations=3,  # Reduced to catch issues faster\n",
    "    early_stopping_method=\"generate\"  # Better handling when max iterations reached\n",
    ")\n",
    "\n",
    "# 4. Test the agent with a natural question\n",
    "try:\n",
    "    result = agent.run(\"Do I need an umbrella in Seattle today?\")\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Final Result:\")\n",
    "    print(result)  # agent.run() returns a string, not a dict\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError with ReAct agent: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Make sure llama3.2:3b is installed: ollama pull llama3.2:3b\")\n",
    "    print(\"2. Make sure Ollama server is running: ollama serve\")\n",
    "    print(\"3. Check that the model has enough memory to run\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed9e8fb",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "\n",
    "The console log displays the **Thought → Action → Observation → …** loop until the agent produces its final answer. Because `verbose=True`, LangChain prints each intermediate reasoning step.\n",
    "\n",
    "If you want to add more tools, simply append them to the tools list. LangChain will handle argument validation, schema generation, and tool-calling logic automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5- Perplexity‑Style Web Search\n",
    "Agents become much more powerful when they can look up real information on the web instead of relying only on their internal knowledge.\n",
    "\n",
    "In this section, you will combine everything you have learned to build a simple Ask-the-Web Agent. You will integrate a web search tool (DuckDuckGo) and make it available to the agent using the same tool-calling approach as before.\n",
    "\n",
    "This will let the model retrieve fresh results, reason over them, and generate an informed answer—similar to how Perplexity works.\n",
    "\n",
    "You may find some examples from the following links:\n",
    "- https://pypi.org/project/duckduckgo-search/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 1: Add a web search tool\n",
    "# ---------------------------------------------------------\n",
    "# Goal:\n",
    "#   Create a tool that lets the agent search the web and return results.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Use DuckDuckGo for quick, open web searches.\n",
    "#   2. Write a helper function (e.g., search_web) that:\n",
    "#        • Takes a query string\n",
    "#        • Uses DDGS to fetch top results (titles + URLs)\n",
    "#        • Returns them as a formatted string\n",
    "#   3. Wrap it with the @tool decorator to make it available to LangChain.\n",
    "\n",
    "\n",
    "from ddgs import DDGS\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"Search the web for information using DuckDuckGo. Returns top search results with titles and URLs.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query string\n",
    "    \n",
    "    Returns:\n",
    "        A formatted string containing search results with titles and URLs.\n",
    "    \"\"\"\n",
    "    # Strip any quotes that the LLM might have included\n",
    "    query = query.strip().strip(\"'\\\"\")\n",
    "    print(query)\n",
    "    \n",
    "    try:\n",
    "        # Use DDGS to search the web\n",
    "        with DDGS() as ddgs:\n",
    "            results = list(ddgs.text(query, max_results=5))\n",
    "        \n",
    "        # Format the results into a readable string\n",
    "        if not results:\n",
    "            return f\"No search results found for: {query}\"\n",
    "        \n",
    "        formatted_results = f\"Search results for '{query}':\\n\\n\"\n",
    "        for i, result in enumerate(results, 1):\n",
    "            title = result.get('title', 'No title')\n",
    "            url = result.get('href', 'No URL')\n",
    "            snippet = result.get('body', 'No description')\n",
    "            formatted_results += f\"{i}. {title}\\n   URL: {url}\\n   {snippet}\\n\\n\"\n",
    "        \n",
    "        return formatted_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error searching the web: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 2: Initialize the web-search agent\n",
    "# ---------------------------------------------------------\n",
    "# Goal:\n",
    "#   Connect your `web_search` tool to a language model\n",
    "#   so the agent can search and reason over real data.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Import `initialize_agent` and `AgentType`.\n",
    "#   2. Create an LLM (e.g., ChatOllama).\n",
    "#   3. Add your `web_search` tool to the tools list.\n",
    "#   4. Initialize the agent using: initialize_agent\n",
    "#   5. Keep `verbose=True` to observe reasoning steps.\n",
    "#\n",
    "# Expected:\n",
    "#   The agent should be ready to accept user queries\n",
    "#   and use your web search tool when needed.\n",
    "# ---------------------------------------------------------\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# Create an LLM instance using ChatOllama\n",
    "llm = ChatOllama(model=\"llama3.2:3b\", temperature=0)\n",
    "\n",
    "# Add the web_search tool to the tools list\n",
    "web_tools = [web_search]\n",
    "\n",
    "# Initialize the agent with the web search tool\n",
    "web_agent = initialize_agent(\n",
    "    tools=web_tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    max_iterations=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature controls the randomness of the LLM's responses:\n",
    "\n",
    "Temperature = 0: Makes the model completely deterministic - it always picks the most likely next token. This gives consistent, predictable outputs.\n",
    "Higher temperature (e.g., 0.7-1.0): Increases randomness and creativity, making responses more varied but potentially less focused.\n",
    "For tool-calling agents, temperature=0 is preferred because:\n",
    "\n",
    "Consistent reasoning: The ReAct agent needs to reliably follow the Thought → Action → Observation pattern\n",
    "Reliable tool calls: You want the agent to consistently format tool calls correctly (JSON syntax matters)\n",
    "Predictable behavior: When dealing with real APIs and external tools, deterministic behavior makes debugging easier\n",
    "Factual accuracy: Lower temperature reduces hallucination risk when the agent needs to be precise\n",
    "You could increase the temperature (e.g., to 0.3-0.5) if you want more creative responses in the final answer, but for structured tasks like tool calling, keeping it at 0 is the standard practice.\n",
    "\n",
    "\n",
    "Let’s see the agent's output in action with a real example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To find the current events in San Francisco for this week, I should use the web_search tool.\n",
      "\n",
      "Action: web_search\n",
      "Action Input: \"current events in San Francisco this week\"\u001b[0mcurrent events in San Francisco this week\n",
      "\u001b[32;1m\u001b[1;3mThought: To find the current events in San Francisco for this week, I should use the web_search tool.\n",
      "\n",
      "Action: web_search\n",
      "Action Input: \"current events in San Francisco this week\"\u001b[0mcurrent events in San Francisco this week\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mSearch results for 'current events in San Francisco this week':\n",
      "\n",
      "1. ABC7 News - KGO Bay Area and San Francisco News\n",
      "   URL: \n",
      "   No one covers SanFrancisco weather and the surrounding Bay Area like ABC7. KGO covers forecasts, weather maps, alerts, video, street-level weather and more.Watch live streaming video on ABC7news.com and stay up-to-date with the latest KGO news broadcasts as well as live breaking news whenever it happens.ABC7 covers local SanFrancisco news, weather, sports and traffic so you stay up-to-date. Stay informed on your region with breaking news and streaming video.Watch live streaming video on ABC7news.com and stay up-to-date with the latest KGO news broadcasts as well as live breaking news whenever it happens.\n",
      "\n",
      "2. 12 Fun Things To Do In San Francisco This Weekend: November 28-30\n",
      "   URL: https://secretsanfrancisco.com/things-to-do-this-weekend-sf/\n",
      "   \n",
      "\n",
      "3. Breaking News from KPIX-TV - CBS San Francisco\n",
      "   URL: https://www.cbsnews.com/sanfrancisco/local-news/\n",
      "   Get the latest news and headlines from KPIX-TV CBS5 SanFrancisco.\n",
      "\n",
      "4. Events this week in San Francisco, CA - Eventbrite\n",
      "   URL: https://www.eventbrite.com/d/ca--san-francisco/events--this-week/\n",
      "   \n",
      "\n",
      "5. News — Breaking Bay Area news, U.S. and World News ...\n",
      "   URL: https://www.sfgate.com/news/\n",
      "   First responders and medical personnel were on the scene. The weather forecast for 2025 is much different compared with recent years. An earthquake with a preliminary magnitude of 4.1 shook 37...\n",
      "\n",
      "\u001b[0m\n",
      "Thought:\n",
      "Observation: \u001b[36;1m\u001b[1;3mSearch results for 'current events in San Francisco this week':\n",
      "\n",
      "1. ABC7 News - KGO Bay Area and San Francisco News\n",
      "   URL: \n",
      "   No one covers SanFrancisco weather and the surrounding Bay Area like ABC7. KGO covers forecasts, weather maps, alerts, video, street-level weather and more.Watch live streaming video on ABC7news.com and stay up-to-date with the latest KGO news broadcasts as well as live breaking news whenever it happens.ABC7 covers local SanFrancisco news, weather, sports and traffic so you stay up-to-date. Stay informed on your region with breaking news and streaming video.Watch live streaming video on ABC7news.com and stay up-to-date with the latest KGO news broadcasts as well as live breaking news whenever it happens.\n",
      "\n",
      "2. 12 Fun Things To Do In San Francisco This Weekend: November 28-30\n",
      "   URL: https://secretsanfrancisco.com/things-to-do-this-weekend-sf/\n",
      "   \n",
      "\n",
      "3. Breaking News from KPIX-TV - CBS San Francisco\n",
      "   URL: https://www.cbsnews.com/sanfrancisco/local-news/\n",
      "   Get the latest news and headlines from KPIX-TV CBS5 SanFrancisco.\n",
      "\n",
      "4. Events this week in San Francisco, CA - Eventbrite\n",
      "   URL: https://www.eventbrite.com/d/ca--san-francisco/events--this-week/\n",
      "   \n",
      "\n",
      "5. News — Breaking Bay Area news, U.S. and World News ...\n",
      "   URL: https://www.sfgate.com/news/\n",
      "   First responders and medical personnel were on the scene. The weather forecast for 2025 is much different compared with recent years. An earthquake with a preliminary magnitude of 4.1 shook 37...\n",
      "\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: What are the current events in San Francisco this week?\n",
      "Thought: I now know the final answer\n",
      "Final Answer: Here are some current events happening in San Francisco this week:\n",
      "\n",
      "1. ABC7 News - KGO Bay Area and San Francisco News (https://abc7news.com/)\n",
      "2. 12 Fun Things To Do In San Francisco This Weekend: November 28-30 (https://secretsanfrancisco.com/things-to-do-this-weekend-sf/)\n",
      "3. Breaking News from KPIX-TV - CBS San Francisco (https://www.cbsnews.com/sanfrancisco/local-news/)\n",
      "4. Events this week in San Francisco, CA - Eventbrite (https://www.eventbrite.com/d/ca--san-francisco/events--this-week/)\n",
      "5. News — Breaking Bay Area news, U.S. and World News ... (https://www.sfgate.com/news/)\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "==================================================\n",
      "Final Result:\n",
      "Here are some current events happening in San Francisco this week:\n",
      "\n",
      "1. ABC7 News - KGO Bay Area and San Francisco News (https://abc7news.com/)\n",
      "2. 12 Fun Things To Do In San Francisco This Weekend: November 28-30 (https://secretsanfrancisco.com/things-to-do-this-weekend-sf/)\n",
      "3. Breaking News from KPIX-TV - CBS San Francisco (https://www.cbsnews.com/sanfrancisco/local-news/)\n",
      "4. Events this week in San Francisco, CA - Eventbrite (https://www.eventbrite.com/d/ca--san-francisco/events--this-week/)\n",
      "5. News — Breaking Bay Area news, U.S. and World News ... (https://www.sfgate.com/news/)\n",
      "\u001b[32;1m\u001b[1;3mQuestion: What are the current events in San Francisco this week?\n",
      "Thought: I now know the final answer\n",
      "Final Answer: Here are some current events happening in San Francisco this week:\n",
      "\n",
      "1. ABC7 News - KGO Bay Area and San Francisco News (https://abc7news.com/)\n",
      "2. 12 Fun Things To Do In San Francisco This Weekend: November 28-30 (https://secretsanfrancisco.com/things-to-do-this-weekend-sf/)\n",
      "3. Breaking News from KPIX-TV - CBS San Francisco (https://www.cbsnews.com/sanfrancisco/local-news/)\n",
      "4. Events this week in San Francisco, CA - Eventbrite (https://www.eventbrite.com/d/ca--san-francisco/events--this-week/)\n",
      "5. News — Breaking Bay Area news, U.S. and World News ... (https://www.sfgate.com/news/)\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "==================================================\n",
      "Final Result:\n",
      "Here are some current events happening in San Francisco this week:\n",
      "\n",
      "1. ABC7 News - KGO Bay Area and San Francisco News (https://abc7news.com/)\n",
      "2. 12 Fun Things To Do In San Francisco This Weekend: November 28-30 (https://secretsanfrancisco.com/things-to-do-this-weekend-sf/)\n",
      "3. Breaking News from KPIX-TV - CBS San Francisco (https://www.cbsnews.com/sanfrancisco/local-news/)\n",
      "4. Events this week in San Francisco, CA - Eventbrite (https://www.eventbrite.com/d/ca--san-francisco/events--this-week/)\n",
      "5. News — Breaking Bay Area news, U.S. and World News ... (https://www.sfgate.com/news/)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 3: Test your Ask-the-Web agent\n",
    "# ---------------------------------------------------------\n",
    "# Goal:\n",
    "#   Verify that the agent can search the web and return\n",
    "#   a summarized answer based on real results.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Ask a natural question that requires live information,\n",
    "#      for example: \"What are the current events in San Francisco this week?\"\n",
    "#   2. Call agent.\n",
    "#\n",
    "# Expected:\n",
    "#   The agent should call `web_search`, retrieve results,\n",
    "#   and generate a short summary response.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "try:\n",
    "    # Ask a question that requires current web information\n",
    "    result = web_agent.run(\"What are the current events in San Francisco this week?\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Final Result:\")\n",
    "    print(result)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError running web agent: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Make sure Ollama server is running: ollama serve\")\n",
    "    print(\"2. Verify duckduckgo-search is installed: pip install duckduckgo-search\")\n",
    "    print(\"3. Check your internet connection\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6- A minimal UI\n",
    "This project includes a simple **React** front end that sends the user’s question to a FastAPI back end and streams the agent’s response in real time. To run the UI:\n",
    "\n",
    "1- Open a terminal and start the Ollama server: `ollama serve`.\n",
    "\n",
    "2- In a second terminal, navigate to the frontend folder and install dependencies:`npm install`.\n",
    "\n",
    "3- In the same terminal, navigate to the backend folder and start the FastAPI back‑end: `uvicorn app:app --reload --port 8000`\n",
    "\n",
    "4- Open a third terminal, navigate to the frontend folder, and start the React dev server: `npm run dev`\n",
    "\n",
    "5- Visit `http://localhost:5173/` in your browser.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 🎉 Congratulations!\n",
    "\n",
    "* You have built a **web‑enabled agent**: tool calling → JSON schema → LangChain ReAct → web search → simple UI.\n",
    "* Try adding more tools, such as news or finance APIs.\n",
    "* Experiment with multiple tools, different models, and measure accuracy vs. hallucination.\n",
    "\n",
    "\n",
    "👏 **Great job!** Take a moment to celebrate. The techniques you implemented here power many production agents and chatbots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
